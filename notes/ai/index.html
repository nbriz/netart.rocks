<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Machine Learning for Artists</title>
    <meta name="author" content="Nick Briz">
    <meta charset="utf-8">

    <!-- social media stuff -->
    <meta property="og:title" content="AI Notes">
    <meta property="og:image" content="https://netart.rocks/images/netartdiagram.gif">
    <meta name="twitter:card" content="summary_large_image">
    <meta property="og:site_name" content="https://netart.rocks">
    <meta name="twitter:site" content="@nbriz">

    <link rel="icon" type="image/png"  href="https://netart.rocks/images/icon.png">
    <link rel="stylesheet" href="/css/styles.css">
    <style media="screen">

      .border {
        border: 3px solid blue;
        padding: 20px;
        border-radius: 3px;
      }

    </style>
  </head>
  <body>

    <header>
      <h1>Machine Learning (AI) + Artists</h1>
    </header>

    <blockquote cite="https://www.youtube.com/watch?v=qGoCUpa8Qbos">
      <p>"As AI progresses, the great promise is that these machines alongside of us are able to think, imagine and see things in ways that we never have before. Which means that maybe we have some new, weird, seemingly implausible solution to climate change, maybe we have some radically different approach to dealing with incurable cancers. The real practical and wonderful promise is that machines help us be more creative and using that creativity we get to terrific solutions."</p>
      <cite>
        <a href="https://www.youtube.com/watch?v=qGoCUpa8Qbo" target="_blank">Amy Webb</a>
      </cite>
    </blockquote>

    <article>

      <iframe class="rwd" height="420" src="https://www.youtube.com/embed/G2XdZIC3AM8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

      <p style="font-style: italic; font-size: 1rem;">
        For more on Kate Crawford and Trevor Paglen's research checkout their online essay  <a href="https://excavating.ai/" target="_blank">excavating.ai</a> on "the politics of images in machine learning training sets". For a critical response to Refik Anadol's MoMA piece "Unsurpervised" checkout this review from <a href="https://www.e-flux.com/criticism/527236/refik-anadol-s-unsupervised" target="_blank">e-flux</a>.
      </p>

      <br><br><br><br>

      <h3>Making Art with AI</h3>
      <h4>(high-level to low-level)</h4>

      <ul>
        <li>
          <b>AI as muse</b>: Google experimented with Language Models and writers in this "<a href="https://www.youtube.com/watch?v=iXTG8ZiLs1s" target="_blank">Digital Muse</a>" project. That was 3 years ago, the language models we have today are already orders of magnitude larger (in size/scope/etc), but the principle is still the same: use AI as part of your creative process (brainstorming, concepting, etc). The tools (apps) we build around these AI models might encourage this way of working, for example Google's <a href="https://aitestkitchen.withgoogle.com/tools/text-fx" target="_blank">TextFX</a> made in collaboration with the rapper <a href="https://www.youtube.com/watch?v=yYp18JAvKkQ" target="_blank">Lupe Fiasco</a>, or might not (ex: <a href="https://chat.openai.com/" target="_blank">ChatGPT</a>). The same can be said for image generating models like <a href="https://openai.com/product/dall-e-2" target="_blank">Dall-E</a>, <a href="https://github.com/Stability-AI/stablediffusion" target="_blank">Stable Diffusion</a>, <a href="XXX" target="_blank">midjourney</a>, etc as well as music tools like <a href="https://openai.com/blog/jukebox/" target="_blank">Jukebox</a> and the various <a href="https://magenta.tensorflow.org/demos" target="_blank">Magenta</a> projects.
        </li>
        <iframe class="rwd" height="420" src="https://www.youtube.com/embed/iXTG8ZiLs1s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <br><br>
        <li>
          <b>AI as studio assistant</b>: the quality of writing/sounds/images that these models (like the Dall-E, <a href="https://openai.com/blog/chatgpt/" target="_blank">ChatGPT</a>, etc) are starting to produce are good enough these days to make it to the final product (edited or even readymade), we've been seeing a lot of that form "AI artists" using prompt-based systems to create NFTs for example, but we’re also starting to see new AI-powered studio tools (think Adobe apps), like <a href="https://www.nvidia.com/en-us/studio/canvas/" target="_blank">NVIDIA Canvas</a>, <a href="https://runwayml.com/" target="_blank">RunwayML</a>, <a href="https://ebsynth.com/" target="_blank">EbSynth</a>, <a href="https://www.descript.com/" target="_blank">descript</a> and so many others starting to come out.
        </li>
        <iframe class="rwd" height="420" src="https://www.youtube.com/embed/wKztRskmsig" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <br><br>
        <li>
          <b>AI powered art</b>: new media artists and creative technologists are starting to make work which incorporates pre-existing AI models, like this <a href="https://arstechnica.com/information-technology/2023/11/unauthorized-david-attenborough-ai-clone-narrates-developers-life-goes-viral/" target="_blank">playful mashup</a> of GPT-4's vision model and ElevenLab's voice models made by Charlie Holtz or this <a href="https://experiments.withgoogle.com/billtjonesai" target="_blank">dance project by Bill T. Jones</a> in collaboration with Google is maybe a less obvious example.
        </li>
        <iframe class="rwd" height="420" src="https://www.youtube.com/embed/RVyh1ewep84" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

        <p class="border">
          To experiment with this sort of AI right now, check out this netnet sketch which uses an AI "pose detection" model to track your position via your webcam and places an animated <a href="https://netnet.studio/?layout=dock-left#code/eJytV11v2zYUfa5/xa33ILuzpGzAXoI2WNZ2QIEWLbBuL8MwUNKVzEQiNZKK4xb57zskJdlO2g7F+uBYFu8994PnfuTp4zSl91u2TMIwuS1TKwsjjGRLkkrd7+l76oV1XFFtdBdEfmnFB36noVTpcuhYOeGkVpBrmNL0YvHUw26d6+15njfSbYciK3WXO1ZWm7rVu9zVVzbtdMWtzZ1hzjtvxOQ9YNOKHZceMremzAtvzr//u+NKil72oxVbGtk7gsyz5WStrFR2ZQErb0ym2OWq7/KfZ8WAv7x4mkfdrwK5732pzbeCKkR5zapKd1w07f/GnPJ6mstPwF4siPIn1LS6EC158ReTND3Jcdiyo0bWG4oo2mzoRlasceRVc9BBWqoHFXVKw8KBOSJKEbfs2UG7rSy3ZHHPorOkB0MItBQd1QxeCYVPVYFwjpwODPNMigZ2YKa9htTbntXlq8SClBXfesGdkY7vebCKpnaBy4oM18K77a1QId06gk6J7Fvham26TANcyMDRAJ9eiRsRc5Ra+FfoW2gKu1flwRZ8/sOHuVrTR5wSPFPWjaE/m2sji1l5GXOxSsJ5sg4a4Tmz7C6dM7IYHK8SMTgNx/bJhpIviHX4W/2HjIexUrVS8ZHk7Fihq30mesRePd/KtloFjPVRLPHKEIzYCdyOEjeyEchnFgrqBd/Ikm3WsPvdsnnj360+BpBzcmbguxPXTPm2uAKLADfiUqAR4ZrcYNRMrbvP0KvVAizx/Lh8RYHjDy8FCRj6N/6M5nvxN85ob2BFL8vrkY694dQZgdxUEcyf74QKJBwsH6UhHj87rZDst6HvtcEtBHM2m9vifaPBJ7K6Y1oCsJbNYELHXPojJ1Vjj4xNpfY8SMJqDAJZGpSTHZ9TMrezZDOeWd0OHvGdcFsIfE03TALE3eQ0/EUZFEhLyLTbaZ+PyOEp90v46F1cznn/hP8za06zFpFejEKrkNrNvaDXx6yYjk6I4XtIZcTuqPIlOoWSnRipovv1Q3YElWNeINpaGniNCw/Rzd4jzFkNCQBbKgnuesPLMESoEBbU8cfQi1VfG9Hx4tGjmAkvZuc0TMgZW+fdDFSxxzUna1oFnaxl1bgtXdAZfPV+4mjMfYSxETzLspEAMZbGiCIIxqCued9rCUbPozu+78PsDkBcbQiZC2EpH9YJXil8RfFYMROc9X2VTejBsZSEQWaiiwFbxqSg09jzA+C32Ai+m7eBdHYnxeVVqVTpTOx0lgrWx/vw8Y1VbP88+yubAfDj4GWQ0rFTgUbXFivRNaruI92ekxq6gjEF94fHD4dH61eCw08lfLXGtI4FFotEt5y1uln5kzUdTFvtkz5R0Z9mt2E8hsd9KMStUA1PmZaBnmPmMahjtHjIrNt7I1z7djsipV4k28kK1Mrpx+wn7HZJf5uMDhzUMIAmrf2otWXZbN1DtbuxVP8ZQOvLqf5+9XWw8tW2DmW7+Eyjvl+LY5sR3uS0PSxG374wUmXXjLMtxGBKyOIllggLstUWfTDDd7I4DXNOIKRFEVro2A0/PyKhHS1NXoc94+C6L6ST5ecwAedOcFgcvgBy1H8g9ehBVz0Muocw4Na4THXiGvNnGHf7OMf8GEXjAlcNINv93LsnWiazMbhT+UVkerFePAw8hgR/AFFRwVioYNKhJWCwhVBO23LQh/fvMcv04AJLNvTD2Rl63TzzpV8it9iKH0/m0OoQD1YN/KeiB1jyAS2NKP28UlW4SLuEoh04co5oJ1Wldxny/fIGV/FaoqEoNqvEpwBhhRyuF4t5Mf4Xi41aWA==" target="_blank">Gif over your face</a>, or this technically (in terms of the code) similar example, where rather than using pose detection to move a gif around the screen, we're using it to control a <a href="https://netnet.studio/?layout=dock-left#code/eJytV2tv2zYU/Vz/ilsPqO3OktyiG9ogCZY1HVBgRQus25eiGCiJtplIpEZSUb0i/33nkrJs59GiWIHED/Hec89908cPk4Ter6WTJKwkv5ZUqdwKq6QjRYVpNvQjNcJ5WdLSmjqI/FqJf+U7A6XSFG0ttRdeGQ25laQkOR0dM+za+8YdZdlK+XWbp4WpMy+1M3ZZmS7zywuX1KaUlcu8lTKr2YjNGsAmpfSyYMjM2SLL2Rw//7uWpRKNanorrrCq8QSZk/HWWlHq9MIBVl3ZVEuf6abOfhkUA/749DiLut8EcpN9Yez3gspFcSl1mXQyX1X/G3Mb18NYHsCGBEmk/PWkJlU3xnqlVyG7742WgO8LYUPCUSerit9hC38TB9WqkXYQuTcbHlgXLu1LQJksb1VVZr2Jex3VOq2VvlvidESUPaZVZXJREbt4vvVwHrjPSWt6nEGskp6i+8bO6UqV0szJuAJHDJLBXeVo2eqgTYWVwqPuRRQlWUmuberWqliTQ5WK2pFpLeKRF6KmpURXCI3/skS7ePImRJD7IBro0FfuElJvG6nPXiNyBVLziQU7q7y8wWAaTXWhEzVZuRTMna1Qrvwsgm6D21TCL42tUwNwoUKHBfjkQlyJGK3EgV9uPkFTuI0udrbA+S92czqjzzglMNPO966fDJ2dxqi8irGYTsL5ZBY0wufUSX/mvVV56+V0IlpvQGwzmdPkC2I1XsuvyDCMU7pSWu5JDsRyU25S0cD38uUaVTUNGLM9X2LK4IzoBLKjxZVaCcQzDePgXF6pQqI4pf/TSfuGn00/B5Aj8raV1wfUbPE2v0ApAa7HpVBGhDT51uoohwfX95RXZQSqhOvj7DWFDr2dFASgbd7wGQ154Yxzp6IqGlVc9uXYWJl4KxCbMoLxeSd0KMLWyb0wxOOTw15J/2gb7npZBnMuHYb6TaOBEzlTSxoDcKlWrQ3zfsxHPDbcnrFtv70MkrAanUCUWu1VLY9oMgzjybw/c6ZqGfGd8GsIfMssnwSI6y1p8EUb5AhLiLTvDMcj1vA29mNwZIrjIe538B+q5jBqEem8F5qG0M5vOD3br4rt0UFh8Awprej2Ol9hUmhVi75UTDO7XR1BZb8u4O1SWbBGwoN3A3u4OaghAKiWUqF22fA4rEDKhUPp8DH0Ytcvrajl6MGDGAkWc0MYtsipdJ5phlJx+z2nljQNOmkl9cqv6ZQW4Mo8cdTHPsK4CJ6maV8A0ZeVFXkQfPKCppVc+r+VxjibhRn7dEFTq1br4eGl3DRGae8OMArBXST7LhlkeJZiYfHcje0jLKIRaTEXUjEQmC7uaAf4Pe4wPwz3l2SgkyBhZaJ0MhRzMkgF630OOAh957oPi4/pAPDhyYuPTDAIhICgEvRK2p57VA/xulv/6SLoR4k7APo3TirbSDd0ekILevSI+q/HJ/Ts+WI29PewQAzPGt7goulV57SYs/CckicLvP4063UeYBmnUMBAT69E1Upo4mt/eo0NjMzsDNwhzYBb8UPmkc3Syn92dIK36afA5+dnTIpfnsON0c4Cq7RSF5vBCD8Zje6gFEu747LREx/qLoR69HW+1/2QgCnnz7ad/xt34JT7fBYGxuieFXFzCvQDbor05HKFWubdiXTODm50b12hqkrEaRTocWhkFwV2p9NZ6s05N7oOrPDdeTTMlEndlTP2ipmAG65BuOXEc4Sl05Rv0PVL0VZ+NKQnLEfccHbUeTIdXLt2u3eYQbsryxdA9iYfs701z3cr9jZMZVb9Na4Wl9h8bf+bKG5QXuAYmbj2W0BWm2FrmEqmUJ1OBmOgU/IVaPtgNrrteHQJfABRIm24ysEkx5lv4uzK4UII+mD/HlvUtD5UyZwQeTThcNtQfH1d49fEw605DFn4g0sOfuGZFpbYobEVBW9KXSrGd2MoulbGmiPqMBBMlyLer66Qit8VxpqWdjrhEMCtEMPZaDRczv8DkCCxbA==" target="_blank">digital theramin</a> made with Tone.js.
        </p>
        <br><br>
        <li>
          <b>AI as an artwork</b>: some artists have been training their own modesl, like Holly Herndon who trained an AI of ver voice called <a href="https://holly.mirror.xyz/54ds2IiOnvthjGFkokFCoaI4EabytH9xjAYy1irHy94" target="_blank">Holly+</a> (she’s got a great <a href="https://www.youtube.com/watch?v=5cbCYwgQkTE" target="_blank">TED talk/performance</a> on it) or <a href="https://www.moma.org/calendar/exhibitions/5535" target="_blank">Refik Anadol</a>'s image generator trained on the MoMA's collection.
          <br><br>
          <iframe class="rwd" height="420" src="https://www.youtube.com/embed/5cbCYwgQkTE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          <br><br>
          This might sound hard at first, but there's actually a few ways to do this:
          <br><br>
          <ul>

            <li>
              You can take an existing model (pre-trained AI) and build on top of it. You can learn the basics here: <a href="https://teachablemachine.withgoogle.com/" target="_blank">https://teachablemachine.withgoogle.com</a> I think this is also a very accessible demo we can use to better understand how these models are “trained”, what their limits are, how bias enters the picture, etc. Also check out the <a href="https://ml5js.org/" target="_blank">ml5.js</a> library and the corresponding tutorial series <a href="https://thecodingtrain.com/tracks/ml5js-beginners-guide" target="_blank">A Beginner's Guide to Machine Learning in JavaScript</a> from the Coding Train.
            </li>
            <li>
              You can gather your own data (or use publicly available datasets like those found on <a href="https://www.kaggle.com/" target="_blank">kaggle.com</a> or <a href="https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit" target="_blank">elsewhere</a>) and use existing neural network architecture to train your own models from scratch. This typically takes a lot of data and processing power (so it can be take a while to create), artist Gene Kogan has some great resources for this called <a href="https://ml4a.net/" target="_blank">Machine Learning for Artists</a>.
            </li>
            <li>
              Or you could create your own architecture and train your own models “from scratch”, or rather, using now industry standard Machine Learning libraries like <a href="https://www.tensorflow.org/js/" target="_blank">Tensor Flow</a> (see <a href="https://www.youtube.com/playlist?list=PLOU2XLYxmsILr3HQpqjLAUkIPa5EaZiui" target="_blank">Machine Learning for Web Devs and Creatives</a> by Google)
            </li>
          </ul>
        </li>
      </ul>

      <h3>Making Art about AI</h3>

      <blockquote cite="https://www.youtube.com/watch?v=8a-Tp0DFJEA">
        <p>The coded gaze reflects the priorities, preferences and prejudices of those who have the power to shape technology</p>
        <cite>
          <a href="https://www.youtube.com/watch?v=8a-Tp0DFJEA" target="_blank">Joy Buolamwini</a>
        </cite>
      </blockquote>

      <iframe class="rwd" height="420" src="https://www.youtube.com/embed/Sqa8Zo2XWc4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

      <p>
        AI isn't just changing our field, it's reshaping so many different aspects of our lives. I also think it's important to engage with some of these technologies not necessarily so to AI-art (or AI-aided art) but work in any medium <i>about</i> this present moment.
      </p>

      <div class="grid" style="grid-template-columns: 1fr 2fr;">
        <a href="https://drib.net/the-treachery-of-imagenet" target="_blank"><img src="/images/notes/ai/this-is-not-a-forklift.png" alt="an abstract composition designed to trick AI image classifiers by Tom White"></a>

        <a href="https://anatomyof.ai/" target="_blank">
          <img src="/images/notes/ai/ai-anatomy-map.png" alt="a detailed diagram of the human labor, data and planetary resources used in Amazon Echo's AI system">
        </a>
      </div>

      <p>
        One example is Kate Crawford and Vladan Joler's 2018 piece <a href="https://anatomyof.ai/" target="_blank">Anatomy of an AI System</a> (seen above), a detailed diagram, with an accompanying essay, of the human labor, data and planetary resources used in Amazon Echo's AI system. The work itself is a PDF (often printed in large format for exhibitions) with an accompanying essay, which is to say that it is not "digital art" (in terms of the medium), but the work is clearly informed by the artists's deep research into the subject matter of AI.
      </p>
      <p>
        Another example might be Tom White’s <a href="https://drib.net/the-treachery-of-imagenet" target="_blank">The Treachery of ImageNet</a> (seen above), a reference to Magritte’s “this is not a pipe” painting, these prints clearly don’t look like the thing they are not to us (humans) but if you point an AI image classifier at it, it will think it is the thing that it is not. White’s work is an interesting conceptual response to this tech, but there is also loads of political work in this area as well. For example, artists who've worked on "datasets" as art, these aren't necessarily meant to be functional ML training datasets (like the kind you find on kaggle.com) but rather are works aimed at invoking discussions around the role curated data plays in this new world:
      </p>

      <img src="/images/notes/ai/black-health.webp" alt="black-health">

      <p>
        <a href="https://bomani.rip/black-health-book" target="_blank">Black Health</a> by Bomani Oseni McClendon
      </p>

      <img src="/images/notes/ai/datasorting.jpeg" alt="feminist dataset">

      <p>
        <a href="https://carolinesinders.com/feminist-data-set/" target="_blank">Feminist Data Set</a> by Carolin Sinders
      </p>


      <img src="/images/notes/ai/missing-datasets.jpg" alt="missing dataset">

      <p>
        <a href="https://mimionuoha.com/the-library-of-missing-datasets" target="_blank">The Library of Missing Datasets</a> by Mimi Ọnụọha
      </p>

      <br><br>

      <p>
        Of course, a fundamental understanding of how this tech works and more importantly, whose in control of this tech and how is it being used in the world as well as what the implications are, are prerequisites for making meaningful work in this regard. You can find more of my notes on <a href="https://nickbriz.com/talks/bias-in-ai/" target="_blank">Bias in AI here</a>.
      </p>

      <iframe class="rwd" height="420" src="https://www.youtube.com/embed/zl9y8tg7MA0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


    </article>


    <br><br><br><br><br><br>

    <script src="/js/Averigua.js"></script>
    <script src="/js/rainbow-text.js"></script>
    <script src="/js/fancy-chars.js"></script>
    <script>
      FancyText.animate(document.querySelector('header > h1'), 200, 0.2)

      Array.from(document.querySelectorAll('h3'))
        .forEach(h3 => FancyText.animate(h3, 5000, 0.2))

      Array.from(document.querySelectorAll('a'))
        .filter(a => a.className !== 'button')
        .forEach(a => RainbowText.hoverShimmer(a))
    </script>
  </body>
</html>
